"""
Main training module for zero-inflated count models.

This module orchestrates the entire pipeline including data loading, feature engineering,
and model training, based on the successful diagnostic process in the /basics folder.

REFACTORED PIPELINE ARCHITECTURE:

1.  Feature Engineering Flow:
    -   Base: 3 raw predictors (X1, X2, X3).
    -   Advanced: 12 features (polynomials and interactions) identified as optimal.
    -   DEPRECATED: SHAP pruning is removed for simplicity and reliability.

2.  Model Architecture:
    -   LightGBM: `LGBMClassifier` for the zero-stage and `LGBMRegressor` for the count-stage.
    -   DEPRECATED: Custom TensorFlow/Keras models are removed due to instability and lower performance.

3.  Two-Stage Zero-Inflated Structure:
    -   Stage 1 (Classification): P(y = 0) using `LGBMClassifier`.
    -   Stage 2 (Regression): E[log1p(y) | y > 0] using `LGBMRegressor`. The log-transform was key.
    -   Final Prediction: Combines both stages, inverting the log transform for the final count prediction.

4.  Hyperparameter Optimization:
    -   DEPRECATED: In-script Optuna optimization is removed.
    -   The pipeline now loads pre-tuned, optimal hyperparameters from a JSON file
        generated by `basics/hyperparameter_tuning.py`.

5.  Performance Targets (Based on our best model):
    -   Zero-Stage AUC: ≥ 0.75
    -   Count-Stage R²: > 0.18
"""

print("Script starting...")
import logging
import time
import os
import sys
import argparse
from pathlib import Path
import numpy as np
import json # Added for hyperparameters
import lightgbm as lgb # Added for LightGBM
from typing import Dict, Any, Optional, Union
import matplotlib.pyplot as plt
import mlflow
import pandas as pd
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Original imports
from config import Config, load_config
# from data import load_and_split_data # Will use a simpler data loading
from data import load_data # Use the simpler loader
from sklearn.model_selection import train_test_split # Use for splitting
# from features import transform_features # Replaced by AdvancedFeatureEngineer

import evaluate

# Use the standalone AdvancedFeatureEngineer, which is more flexible
from advanced_feature_engineering import AdvancedFeatureEngineer

# --- DEPRECATED IMPORTS ---
# These modules are part of the old, overly complex pipeline and are no longer used.
# try:
#     from features.feature_pruning import run_feature_pruning_pipeline, apply_shap_feature_selection
#     FEATURE_PRUNING_AVAILABLE = True
# except ImportError:
#     FEATURE_PRUNING_AVAILABLE = False
# try:
#     from hyperparameter_optimization import (
#         optimize_zero_stage_hyperparameters, 
#         optimize_count_stage_hyperparameters,
#     )
#     HYPEROPT_AVAILABLE = True
# except ImportError:
#     HYPEROPT_AVAILABLE = False
# try:
#     from gpu_enhanced_models import train_enhanced_models, GPUEnhancedZeroInflatedModel
#     ENHANCED_MODELS_AVAILABLE = True
# except ImportError:
#     ENHANCED_MODELS_AVAILABLE = False
# --- END DEPRECATED IMPORTS ---


# Additional imports for enhanced functionality
from sklearn.metrics import precision_recall_curve, roc_curve, mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
import joblib

# Set up root logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('pipeline.log', mode='w')
    ]
)

# Module-level logger
logger = logging.getLogger(__name__)


def setup_logging(config: Config) -> None:
    """
    Set up logging based on configuration.
    
    Parameters
    ----------
    config : Config
        Configuration object
    """
    # Get log level from config
    log_level_str = config.general.log_level
    log_level = getattr(logging, log_level_str)
    
    # Configure root logger
    logging.getLogger().setLevel(log_level)
    
    logger.info(f"Logging configured at level: {log_level_str}")


def setup_mlflow(config: Config) -> None:
    """
    Set up MLflow tracking based on configuration.
    
    Parameters
    ----------
    config : Config
        Configuration object
    """
    if not config.mlflow.enabled:
        logger.info("MLflow tracking disabled")
        return
    
    # Set tracking URI if provided
    if config.mlflow.tracking_uri:
        mlflow.set_tracking_uri(config.mlflow.tracking_uri)
        
    # Set experiment
    mlflow.set_experiment(config.mlflow.experiment_name)
    
    logger.info(f"MLflow tracking enabled: experiment={config.mlflow.experiment_name}")


def profile(func):
    """
    Decorator for profiling function execution time.
    
    Parameters
    ----------
    func : callable
        Function to profile
        
    Returns
    -------
    callable
        Wrapped function
    """
    def wrapper(*args, **kwargs):
        start_time = time.time()
        logger.info(f"Starting {func.__name__}")
        result = func(*args, **kwargs)
        elapsed = time.time() - start_time
        logger.info(f"Finished {func.__name__} in {elapsed:.2f} seconds")
        return result
    return wrapper


def train_two_stage_model(X_train, y_train, zero_stage_params: dict, count_stage_params: dict):
    """
    Trains the two-stage zero-inflated model.

    Args:
        X_train: Training features.
        y_train: Training target.
        zero_stage_params: Hyperparameters for the zero-stage classifier.
        count_stage_params: Hyperparameters for the count-stage regressor.

    Returns:
        A tuple containing the trained zero_model and count_model.
    """
    logger.info("Training zero-stage model (LGBMClassifier)...")
    y_train_zero = (y_train == 0).astype(int)
    
    zero_model = lgb.LGBMClassifier(**zero_stage_params)
    zero_model.fit(X_train, y_train_zero)

    logger.info("Training count-stage model (LGBMRegressor)...")
    # Filter for non-zero counts for the regression model
    X_train_nonzero = X_train[y_train > 0]
    y_train_nonzero = y_train[y_train > 0]
    
    # If no count model params are provided, use an empty dict
    if not count_stage_params:
        logger.warning("No specific hyperparameters found for the count model. Using LightGBM defaults.")
        count_stage_params = {}

    # Ensure the objective is set to quantile for robustness
    if 'objective' not in count_stage_params or count_stage_params['objective'] != 'quantile':
        logger.info("Setting count model objective to 'quantile' for robust regression.")
        count_stage_params['objective'] = 'quantile'
        count_stage_params['alpha'] = 0.5 # Target the median

    count_model = lgb.LGBMRegressor(**count_stage_params)
    count_model.fit(X_train_nonzero, y_train_nonzero)
    
    return zero_model, count_model


@profile
def run_pipeline(config: Config):
    """
    Run the refactored training and evaluation pipeline.
    """
    output_dir = Path(config.general.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # --- 1. Load Data ---
    logger.info("Loading data...")
    df = load_data(config.data.path)
    X = df[config.data.features]
    y = df[config.data.target]

    # --- 2. Feature Engineering ---
    logger.info("Engineering features...")
    feature_engineer = AdvancedFeatureEngineer(**config.feature_engineering.model_dump())
    X_adv = feature_engineer.fit_transform(X)
    logger.info(f"Feature engineering complete. Number of features: {X_adv.shape[1]}")

    # --- 3. Train/Test Split ---
    logger.info("Splitting data into training and testing sets...")
    y_zero = (y == 0).astype(int)
    X_train, X_test, y_train, y_test = train_test_split(
        X_adv, y, test_size=config.training.test_size, random_state=config.training.random_state, stratify=y_zero
    )
    y_zero_train = (y_train == 0).astype(int)
    y_zero_test = (y_test == 0).astype(int)

    # --- 4. Model Training ---
    logger.info(f"Training model of type: {config.training.model_type}")

    if config.training.model_type == 'lightgbm':
        # Load hyperparameters
        logging.info(f"Loading hyperparameters from {config.training.hyperparameters_path}")
        with open(config.training.hyperparameters_path, 'r') as f:
            hyperparams = json.load(f)

        # Extract params for each stage, providing empty dicts as fallbacks
        zero_stage_params = hyperparams.get('zero_stage', {})
        count_stage_params = hyperparams.get('count_stage', {})

        if not count_stage_params:
            logging.warning("Could not find 'count_stage' key in hyperparameters. Benchmark model may be suboptimal.")

        # Train the two-stage model
        zero_model, count_model = train_two_stage_model(
            X_train, y_train,
            zero_stage_params=zero_stage_params,
            count_stage_params=count_stage_params
        )
        
        # Save the two-stage models
        models_dir = os.path.join(config.general.output_dir, 'models')
        os.makedirs(models_dir, exist_ok=True)
        joblib.dump(zero_model, os.path.join(models_dir, 'two_stage_zero_model.pkl'))
        joblib.dump(count_model, os.path.join(models_dir, 'two_stage_count_model.pkl'))
        logger.info(f"Saved two-stage models to {models_dir}")

    else:
        raise NotImplementedError(f"Model type '{config.training.model_type}' not supported.")

    # --- 5. Prediction and Evaluation ---
    logger.info("Evaluating Two-Stage Model...")

    # Predict probabilities for the zero-stage
    y_prob_zero_stage = zero_model.predict_proba(X_test)[:, 1] # Prob of being non-zero

    # Find optimal threshold for classification
    fpr, tpr, thresholds = roc_curve(y_zero_test, y_prob_zero_stage)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    logger.info(f"Found optimal threshold for non-zero probability: {optimal_threshold:.4f}")

    # Predict classes for zero-stage
    y_pred_zero_class = (y_prob_zero_stage >= optimal_threshold).astype(int)

    # Predict counts for the count-stage
    y_pred_count_stage = count_model.predict(X_test)

    # Combine predictions for the final two-stage output
    y_pred_two_stage = np.where(y_pred_zero_class == 1, y_pred_count_stage, 0)
    # Ensure predictions are non-negative
    y_pred_two_stage[y_pred_two_stage < 0] = 0


    # --- 6. Reporting and Visualization ---
    report_path = os.path.join(config.general.output_dir, 'reports', 'evaluation_report.txt')
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    # Generate and save the full report for the two-stage model
    report = evaluate.generate_report(
        y_true=y_test,
        y_pred=y_pred_two_stage,
        y_zero_true=y_zero_test,
        y_zero_prob=y_prob_zero_stage,
        optimal_threshold=optimal_threshold,
        model_name="Two-Stage Hurdle Model",
        zero_model=zero_model,
        count_model=count_model,
        feature_names=feature_engineer.get_feature_names()
    )
    with open(report_path, 'w') as f:
        f.write(report)
    logging.info(f"Evaluation report for Two-Stage model saved to {report_path}")

    # Generate visualizations for the two-stage model
    logger.info("Generating visualizations for Two-Stage Model...")
    evaluate.generate_visualizations(
        y_true_count=y_test,
        y_pred_count=y_pred_two_stage,
        y_true_binary=y_zero_test,
        y_prob_binary=y_prob_zero_stage,
        config=config,
        model_name="Two-Stage Model",
        prefix="two_stage_"
    )
    
    # --- 7. Benchmark Model ---
    logger.info("Training and evaluating benchmark model (Single LGBMRegressor)...")
    
    # Ensure the benchmark also uses quantile regression for a fair comparison
    if 'objective' not in count_stage_params or count_stage_params['objective'] != 'quantile':
        logger.info("Setting benchmark model objective to 'quantile' for robust regression.")
        count_stage_params['objective'] = 'quantile'
        count_stage_params['alpha'] = 0.5 # Target the median

    benchmark_model = lgb.LGBMRegressor(**count_stage_params, random_state=config.training.random_state)
    benchmark_model.fit(X_train, y_train)
    
    # Save the benchmark model
    joblib.dump(benchmark_model, os.path.join(models_dir, 'benchmark_model.pkl'))
    logger.info(f"Saved benchmark model to {models_dir}")

    # Evaluate the benchmark model
    benchmark_metrics = evaluate.evaluate_single_model(benchmark_model, X_test, y_test)
    logging.info(f"Benchmark Model Metrics: R2={benchmark_metrics['r2']:.4f}, RMSE={benchmark_metrics['rmse']:.4f}, MAE={benchmark_metrics['mae']:.4f}")

    # Append benchmark results to the report
    with open(report_path, 'a') as f:
        f.write("\n\n--- Benchmark Model (Single LGBMRegressor) ---\n")
        for metric, value in benchmark_metrics.items():
            f.write(f"{metric.upper()}: {value:.4f}\n")
    logging.info(f"Benchmark report appended to {report_path}")

    # Generate visualizations for the benchmark model
    logger.info("Generating visualizations for Benchmark Model...")
    y_pred_benchmark = benchmark_model.predict(X_test)
    y_zero_benchmark = (y_test == 0).astype(int)
    # Since the benchmark is a single regressor, we don't have a separate probability score for the zero-stage.
    # We can use a placeholder or skip the ROC curve. For now, let's use the predictions to derive a pseudo-probability
    # for visualization purposes, acknowledging it's not a true probability.
    # A simple way is to scale predictions, but a better approach is to skip ROC for this model.
    # Let's create dummy probabilities for the function call, but not interpret the ROC curve.
    y_prob_benchmark_dummy = np.clip(y_pred_benchmark / (y_pred_benchmark.max() + 1e-6), 0, 1)

    evaluate.generate_visualizations(
        y_true_count=y_test,
        y_pred_count=y_pred_benchmark,
        y_true_binary=y_zero_benchmark,
        y_prob_binary=y_prob_benchmark_dummy, # Using dummy probabilities
        config=config,
        model_name="Benchmark Model",
        prefix="benchmark_"
    )

    # Plot feature importance for the benchmark model
    feature_importance_df = pd.DataFrame({
        'feature': feature_engineer.get_feature_names(),
        'importance': benchmark_model.feature_importances_
    })
    evaluate.plot_feature_importance(
        feature_importance_df,
        os.path.join(config.general.output_dir, 'plots', 'benchmark_feature_importance.png')
    )

    logging.info("Pipeline finished successfully.")

def main():
    parser = argparse.ArgumentParser(description="Run the training pipeline.")
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to the configuration file.')
    args = parser.parse_args()

    try:
        config = load_config(args.config)
        setup_logging(config)
        # setup_mlflow(config) # MLflow setup can be re-enabled if needed
        run_pipeline(config)
    except Exception as e:
        logger.critical(f"Pipeline failed with error: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
